{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6557b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "RSEED = 0\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97665f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"Colorectal_Histology.zip\", \"r\") as zObject:\n",
    "    zObject.extractall(path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hmnist_28_28_L.csv\")\n",
    "X = df.drop('label', axis=1)\n",
    "X = X.to_numpy()\n",
    "y = df.label.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4cbc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0000</th>\n",
       "      <th>pixel0001</th>\n",
       "      <th>pixel0002</th>\n",
       "      <th>pixel0003</th>\n",
       "      <th>pixel0004</th>\n",
       "      <th>pixel0005</th>\n",
       "      <th>pixel0006</th>\n",
       "      <th>pixel0007</th>\n",
       "      <th>pixel0008</th>\n",
       "      <th>pixel0009</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel0775</th>\n",
       "      <th>pixel0776</th>\n",
       "      <th>pixel0777</th>\n",
       "      <th>pixel0778</th>\n",
       "      <th>pixel0779</th>\n",
       "      <th>pixel0780</th>\n",
       "      <th>pixel0781</th>\n",
       "      <th>pixel0782</th>\n",
       "      <th>pixel0783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>154</td>\n",
       "      <td>160</td>\n",
       "      <td>95</td>\n",
       "      <td>44</td>\n",
       "      <td>139</td>\n",
       "      <td>184</td>\n",
       "      <td>164</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>73</td>\n",
       "      <td>72</td>\n",
       "      <td>75</td>\n",
       "      <td>152</td>\n",
       "      <td>130</td>\n",
       "      <td>96</td>\n",
       "      <td>133</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>67</td>\n",
       "      <td>90</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127</td>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "      <td>140</td>\n",
       "      <td>170</td>\n",
       "      <td>111</td>\n",
       "      <td>128</td>\n",
       "      <td>117</td>\n",
       "      <td>60</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>143</td>\n",
       "      <td>119</td>\n",
       "      <td>148</td>\n",
       "      <td>140</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>101</td>\n",
       "      <td>106</td>\n",
       "      <td>120</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>102</td>\n",
       "      <td>71</td>\n",
       "      <td>93</td>\n",
       "      <td>120</td>\n",
       "      <td>84</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>141</td>\n",
       "      <td>121</td>\n",
       "      <td>132</td>\n",
       "      <td>110</td>\n",
       "      <td>131</td>\n",
       "      <td>119</td>\n",
       "      <td>99</td>\n",
       "      <td>101</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>121</td>\n",
       "      <td>136</td>\n",
       "      <td>178</td>\n",
       "      <td>192</td>\n",
       "      <td>210</td>\n",
       "      <td>189</td>\n",
       "      <td>149</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120</td>\n",
       "      <td>102</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>86</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>106</td>\n",
       "      <td>128</td>\n",
       "      <td>152</td>\n",
       "      <td>141</td>\n",
       "      <td>133</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>166</td>\n",
       "      <td>...</td>\n",
       "      <td>140</td>\n",
       "      <td>148</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>132</td>\n",
       "      <td>146</td>\n",
       "      <td>125</td>\n",
       "      <td>160</td>\n",
       "      <td>207</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73</td>\n",
       "      <td>98</td>\n",
       "      <td>126</td>\n",
       "      <td>153</td>\n",
       "      <td>116</td>\n",
       "      <td>45</td>\n",
       "      <td>89</td>\n",
       "      <td>124</td>\n",
       "      <td>108</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>101</td>\n",
       "      <td>125</td>\n",
       "      <td>147</td>\n",
       "      <td>115</td>\n",
       "      <td>156</td>\n",
       "      <td>170</td>\n",
       "      <td>114</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>124</td>\n",
       "      <td>132</td>\n",
       "      <td>130</td>\n",
       "      <td>129</td>\n",
       "      <td>141</td>\n",
       "      <td>146</td>\n",
       "      <td>125</td>\n",
       "      <td>117</td>\n",
       "      <td>114</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>126</td>\n",
       "      <td>120</td>\n",
       "      <td>122</td>\n",
       "      <td>127</td>\n",
       "      <td>119</td>\n",
       "      <td>116</td>\n",
       "      <td>143</td>\n",
       "      <td>157</td>\n",
       "      <td>138</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>141</td>\n",
       "      <td>176</td>\n",
       "      <td>171</td>\n",
       "      <td>173</td>\n",
       "      <td>205</td>\n",
       "      <td>229</td>\n",
       "      <td>236</td>\n",
       "      <td>238</td>\n",
       "      <td>231</td>\n",
       "      <td>239</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>208</td>\n",
       "      <td>220</td>\n",
       "      <td>209</td>\n",
       "      <td>171</td>\n",
       "      <td>131</td>\n",
       "      <td>129</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0000  pixel0001  pixel0002  pixel0003  pixel0004  pixel0005  \\\n",
       "0        101        110        154        160         95         44   \n",
       "1         67         66         69         76         80         57   \n",
       "2        127        137        121        140        170        111   \n",
       "3         80         90        101        106        120        100   \n",
       "4        153        141        121        132        110        131   \n",
       "5        120        102         83         73         74         77   \n",
       "6        187        187        189        199        198        179   \n",
       "7         73         98        126        153        116         45   \n",
       "8        124        132        130        129        141        146   \n",
       "9        141        176        171        173        205        229   \n",
       "\n",
       "   pixel0006  pixel0007  pixel0008  pixel0009  ...  pixel0775  pixel0776  \\\n",
       "0        139        184        164        160  ...        103         73   \n",
       "1         46         67         90         77  ...         58         65   \n",
       "2        128        117         60        105  ...         90        100   \n",
       "3         99         66         63         91  ...        131        109   \n",
       "4        119         99        101         91  ...        117        121   \n",
       "5         86         88         89         93  ...        128        103   \n",
       "6        177        174        179        166  ...        140        148   \n",
       "7         89        124        108         89  ...         98        101   \n",
       "8        125        117        114        107  ...        126        120   \n",
       "9        236        238        231        239  ...        158        158   \n",
       "\n",
       "   pixel0777  pixel0778  pixel0779  pixel0780  pixel0781  pixel0782  \\\n",
       "0         72         75        152        130         96        133   \n",
       "1         74         80         81         83         77         75   \n",
       "2        143        119        148        140        193        146   \n",
       "3         97        102         71         93        120         84   \n",
       "4        136        178        192        210        189        149   \n",
       "5        104        106        128        152        141        133   \n",
       "6        142        129        132        146        125        160   \n",
       "7        125        147        115        156        170        114   \n",
       "8        122        127        119        116        143        157   \n",
       "9        208        220        209        171        131        129   \n",
       "\n",
       "   pixel0783  label  \n",
       "0        159      2  \n",
       "1         73      2  \n",
       "2         97      2  \n",
       "3         62      2  \n",
       "4        155      2  \n",
       "5        107      2  \n",
       "6        207      2  \n",
       "7         90      2  \n",
       "8        138      2  \n",
       "9        131      2  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c264168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00679e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101, 110, 154, ...,  96, 133, 159],\n",
       "       [ 67,  66,  69, ...,  77,  75,  73],\n",
       "       [127, 137, 121, ..., 193, 146,  97],\n",
       "       ...,\n",
       "       [ 27,  50,  94, ..., 223, 149,  77],\n",
       "       [108, 113, 116, ..., 132,  93,  83],\n",
       "       [ 67,  74,  67, ..., 121,  92,  77]], shape=(5000, 784))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "010067cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class histo(Dataset):\n",
    "    def __init__(self, X:NDArray[np.int8], y:NDArray[np.int8])->None:\n",
    "        X = torch.tensor(X, dtype=torch.uint8).view(-1, 1, 28, 28)\n",
    "        self.X = X.float()/255\n",
    "        if y is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = y\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx:int)->int:\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_split(X, y, train_fraction:float, val_fraction:float, test_fraction:float)->Tuple[Dataset, Dataset, Dataset]:\n",
    "        assert train_fraction + val_fraction + test_fraction <= 1\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_fraction)\n",
    "\n",
    "        train_data = histo(X_train, y_train)\n",
    "        val_data = histo(X_val, y_val)\n",
    "\n",
    "        return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "646413ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = histo.create_split(X, y, 0.5, 0.5,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ce9b3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.3569, 0.3137, 0.2549, 0.2078, 0.0902, 0.0941, 0.0824, 0.0706,\n",
       "           0.2471, 0.3412, 0.2118, 0.1686, 0.1765, 0.1882, 0.2275, 0.2471,\n",
       "           0.2118, 0.2235, 0.2392, 0.2392, 0.1686, 0.1843, 0.2039, 0.1961,\n",
       "           0.3020, 0.2667, 0.2196, 0.3373],\n",
       "          [0.3137, 0.2863, 0.2627, 0.2471, 0.0863, 0.1020, 0.1098, 0.0863,\n",
       "           0.3059, 0.4314, 0.2157, 0.1725, 0.1059, 0.2078, 0.2667, 0.2510,\n",
       "           0.2784, 0.2706, 0.2235, 0.2275, 0.1412, 0.1882, 0.3451, 0.3294,\n",
       "           0.3176, 0.2784, 0.2706, 0.2510],\n",
       "          [0.3216, 0.2667, 0.3529, 0.1922, 0.0980, 0.0863, 0.0824, 0.0941,\n",
       "           0.2510, 0.2824, 0.2000, 0.2157, 0.2275, 0.2667, 0.2471, 0.1529,\n",
       "           0.1843, 0.2667, 0.2902, 0.2588, 0.2667, 0.4353, 0.4353, 0.3608,\n",
       "           0.2941, 0.3020, 0.2980, 0.3176],\n",
       "          [0.2745, 0.2863, 0.2627, 0.1373, 0.1216, 0.0784, 0.0392, 0.2745,\n",
       "           0.4667, 0.2157, 0.1725, 0.1529, 0.1608, 0.1922, 0.2157, 0.2784,\n",
       "           0.3647, 0.3333, 0.3529, 0.2980, 0.2118, 0.3255, 0.3451, 0.3647,\n",
       "           0.4275, 0.2902, 0.4039, 0.6431],\n",
       "          [0.2667, 0.3686, 0.1922, 0.1373, 0.1529, 0.1020, 0.0706, 0.3765,\n",
       "           0.3412, 0.1529, 0.1922, 0.3137, 0.3333, 0.2549, 0.2784, 0.4314,\n",
       "           0.5451, 0.3686, 0.2745, 0.1765, 0.2784, 0.3176, 0.2980, 0.3373,\n",
       "           0.5216, 0.3020, 0.3373, 0.6157],\n",
       "          [0.3098, 0.2196, 0.1137, 0.1255, 0.1255, 0.0902, 0.1529, 0.2627,\n",
       "           0.2078, 0.3373, 0.2196, 0.2941, 0.3961, 0.2824, 0.2667, 0.3412,\n",
       "           0.3137, 0.2510, 0.2078, 0.2980, 0.3608, 0.3294, 0.4353, 0.6078,\n",
       "           0.3333, 0.2000, 0.1451, 0.4627],\n",
       "          [0.3098, 0.1765, 0.0980, 0.1765, 0.2353, 0.2000, 0.2431, 0.2275,\n",
       "           0.2667, 0.4431, 0.2627, 0.1843, 0.2941, 0.2549, 0.2431, 0.2235,\n",
       "           0.2510, 0.2000, 0.2314, 0.2941, 0.2941, 0.3882, 0.4000, 0.4235,\n",
       "           0.2353, 0.1059, 0.1569, 0.2588],\n",
       "          [0.2588, 0.1647, 0.1294, 0.3020, 0.4000, 0.2863, 0.2980, 0.2667,\n",
       "           0.1608, 0.1843, 0.1961, 0.2078, 0.2784, 0.2118, 0.1882, 0.2824,\n",
       "           0.2941, 0.2353, 0.2392, 0.2745, 0.4196, 0.5490, 0.5176, 0.3373,\n",
       "           0.1765, 0.1529, 0.2706, 0.2471],\n",
       "          [0.2000, 0.1804, 0.1647, 0.1804, 0.1608, 0.1059, 0.1725, 0.1765,\n",
       "           0.2549, 0.3255, 0.3176, 0.3137, 0.2392, 0.2275, 0.1176, 0.2392,\n",
       "           0.2941, 0.2745, 0.3255, 0.4157, 0.5098, 0.7569, 0.6431, 0.2549,\n",
       "           0.1490, 0.2353, 0.2039, 0.1098],\n",
       "          [0.1216, 0.1412, 0.1333, 0.2157, 0.1882, 0.1569, 0.1647, 0.2745,\n",
       "           0.2471, 0.3373, 0.3451, 0.2980, 0.2314, 0.2824, 0.3451, 0.4157,\n",
       "           0.2549, 0.1922, 0.2784, 0.3176, 0.3059, 0.4784, 0.3686, 0.2471,\n",
       "           0.3255, 0.3137, 0.1059, 0.1176],\n",
       "          [0.0941, 0.1412, 0.1176, 0.2706, 0.2039, 0.1529, 0.2588, 0.3333,\n",
       "           0.2078, 0.2078, 0.2353, 0.2000, 0.2431, 0.3137, 0.3294, 0.3137,\n",
       "           0.1451, 0.2627, 0.2510, 0.1176, 0.2078, 0.2863, 0.2588, 0.2627,\n",
       "           0.3137, 0.4353, 0.2078, 0.1529],\n",
       "          [0.0706, 0.1020, 0.1647, 0.2078, 0.2275, 0.2196, 0.3294, 0.2314,\n",
       "           0.2549, 0.2431, 0.1647, 0.2118, 0.2824, 0.4314, 0.4706, 0.1412,\n",
       "           0.2078, 0.3333, 0.2275, 0.0549, 0.2431, 0.3647, 0.3412, 0.1373,\n",
       "           0.1490, 0.3294, 0.1843, 0.1137],\n",
       "          [0.2588, 0.3490, 0.2784, 0.2392, 0.2980, 0.3961, 0.2314, 0.1725,\n",
       "           0.2353, 0.2157, 0.1373, 0.2118, 0.3216, 0.6196, 0.3294, 0.1804,\n",
       "           0.4706, 0.5765, 0.2471, 0.2471, 0.3725, 0.3804, 0.3137, 0.1333,\n",
       "           0.0863, 0.1216, 0.2941, 0.2549],\n",
       "          [0.4667, 0.4627, 0.2039, 0.2549, 0.3098, 0.4824, 0.2824, 0.1922,\n",
       "           0.1333, 0.1216, 0.2275, 0.4627, 0.3059, 0.3569, 0.2314, 0.5333,\n",
       "           0.7725, 0.7843, 0.3882, 0.4235, 0.4078, 0.2471, 0.1922, 0.2392,\n",
       "           0.1765, 0.0745, 0.2157, 0.2784],\n",
       "          [0.2353, 0.2824, 0.1922, 0.1882, 0.2510, 0.2588, 0.2510, 0.1843,\n",
       "           0.3059, 0.2863, 0.3922, 0.4196, 0.3725, 0.2627, 0.3451, 0.5725,\n",
       "           0.5765, 0.4824, 0.4667, 0.5451, 0.3333, 0.1843, 0.2275, 0.2549,\n",
       "           0.1882, 0.1569, 0.2510, 0.2902],\n",
       "          [0.2745, 0.2000, 0.2000, 0.1294, 0.1451, 0.2314, 0.1647, 0.6000,\n",
       "           0.7373, 0.4275, 0.4471, 0.3255, 0.2078, 0.3608, 0.4549, 0.5020,\n",
       "           0.4039, 0.2745, 0.3804, 0.5020, 0.2353, 0.2471, 0.2275, 0.2314,\n",
       "           0.2706, 0.3098, 0.2706, 0.3255],\n",
       "          [0.2745, 0.2471, 0.1647, 0.1882, 0.2627, 0.1373, 0.5020, 0.9176,\n",
       "           0.8824, 0.5647, 0.2471, 0.2471, 0.2824, 0.5216, 0.4157, 0.4000,\n",
       "           0.3608, 0.2235, 0.1765, 0.2902, 0.2549, 0.2118, 0.2588, 0.2863,\n",
       "           0.2471, 0.2078, 0.2510, 0.3137],\n",
       "          [0.3098, 0.2275, 0.2196, 0.2549, 0.2784, 0.1804, 0.4706, 0.8824,\n",
       "           0.8784, 0.5020, 0.1725, 0.2745, 0.3333, 0.4784, 0.5412, 0.4980,\n",
       "           0.4078, 0.2314, 0.2471, 0.2863, 0.2902, 0.2235, 0.2431, 0.2392,\n",
       "           0.2353, 0.2784, 0.3412, 0.3294],\n",
       "          [0.2902, 0.2039, 0.2353, 0.2667, 0.2941, 0.2902, 0.4627, 0.5569,\n",
       "           0.3843, 0.1686, 0.3059, 0.3961, 0.2980, 0.3059, 0.4627, 0.4314,\n",
       "           0.2314, 0.2157, 0.2941, 0.2667, 0.3059, 0.2902, 0.3294, 0.3255,\n",
       "           0.2510, 0.3059, 0.3059, 0.3176],\n",
       "          [0.2392, 0.2275, 0.3176, 0.3608, 0.3216, 0.4784, 0.3608, 0.1098,\n",
       "           0.1294, 0.2275, 0.3412, 0.2353, 0.1804, 0.0706, 0.1922, 0.1961,\n",
       "           0.1373, 0.2196, 0.2471, 0.2431, 0.2118, 0.2471, 0.3843, 0.3843,\n",
       "           0.2431, 0.1765, 0.2510, 0.4000],\n",
       "          [0.2941, 0.2196, 0.2706, 0.2745, 0.2627, 0.5020, 0.1922, 0.1294,\n",
       "           0.2627, 0.2392, 0.2863, 0.2392, 0.1686, 0.1451, 0.2078, 0.3059,\n",
       "           0.2235, 0.2627, 0.2431, 0.1765, 0.1373, 0.2745, 0.4275, 0.2784,\n",
       "           0.1765, 0.1961, 0.3294, 0.3961],\n",
       "          [0.2000, 0.1765, 0.1765, 0.2353, 0.3216, 0.3059, 0.1882, 0.2902,\n",
       "           0.2510, 0.1686, 0.2392, 0.3059, 0.2118, 0.2157, 0.3490, 0.3922,\n",
       "           0.3216, 0.2745, 0.2745, 0.2549, 0.2000, 0.2863, 0.3216, 0.2078,\n",
       "           0.1804, 0.2745, 0.3020, 0.3255],\n",
       "          [0.1569, 0.1647, 0.2745, 0.2235, 0.2549, 0.3294, 0.3412, 0.3294,\n",
       "           0.1843, 0.2157, 0.2392, 0.3059, 0.1412, 0.3373, 0.4157, 0.3216,\n",
       "           0.2431, 0.2706, 0.3569, 0.3804, 0.3608, 0.3176, 0.2824, 0.2980,\n",
       "           0.1882, 0.2549, 0.4118, 0.3176],\n",
       "          [0.1294, 0.1922, 0.3294, 0.1608, 0.2471, 0.3451, 0.4235, 0.2392,\n",
       "           0.1725, 0.3529, 0.4235, 0.1529, 0.2471, 0.3137, 0.2941, 0.2784,\n",
       "           0.2157, 0.3412, 0.2745, 0.4471, 0.7137, 0.4471, 0.2941, 0.3176,\n",
       "           0.1922, 0.2392, 0.3725, 0.1882],\n",
       "          [0.0863, 0.1922, 0.2902, 0.1451, 0.3255, 0.3922, 0.3490, 0.3255,\n",
       "           0.2941, 0.3059, 0.2353, 0.2000, 0.3255, 0.2824, 0.3255, 0.3098,\n",
       "           0.2941, 0.3686, 0.2902, 0.4000, 0.5529, 0.3765, 0.2745, 0.2863,\n",
       "           0.2039, 0.3294, 0.1882, 0.1922],\n",
       "          [0.1843, 0.2392, 0.3137, 0.2588, 0.4275, 0.4157, 0.3529, 0.3333,\n",
       "           0.2157, 0.2549, 0.2471, 0.2902, 0.2784, 0.3373, 0.3922, 0.2863,\n",
       "           0.2706, 0.2510, 0.1765, 0.1686, 0.1647, 0.3059, 0.3843, 0.2431,\n",
       "           0.2196, 0.3059, 0.1608, 0.1412],\n",
       "          [0.4941, 0.3098, 0.2745, 0.3255, 0.4627, 0.2627, 0.3294, 0.1922,\n",
       "           0.1961, 0.2667, 0.3176, 0.3255, 0.3647, 0.4039, 0.2784, 0.2980,\n",
       "           0.2392, 0.1098, 0.1255, 0.0902, 0.2941, 0.4980, 0.3882, 0.2941,\n",
       "           0.3922, 0.3333, 0.2510, 0.1765],\n",
       "          [0.4157, 0.2941, 0.2392, 0.3020, 0.3137, 0.2549, 0.2078, 0.1882,\n",
       "           0.2392, 0.2745, 0.3176, 0.3882, 0.4392, 0.3176, 0.2549, 0.2784,\n",
       "           0.2275, 0.2431, 0.2196, 0.1373, 0.4667, 0.4549, 0.3608, 0.4000,\n",
       "           0.3804, 0.4000, 0.3647, 0.3412]]]),\n",
       " np.int64(1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5353327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  main():\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ViTModel((1, 28, 28), n_patches=16, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "    num_epochs = 4\n",
    "    learn_rate = 0.005\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learn_rate)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch +1} in training\", leave=False):\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item()/len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} loss: {train_loss:.3f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct, total = 0,  0\n",
    "\n",
    "            test_loss = 0.0\n",
    "\n",
    "            for batch in tqdm(val_loader, desc=\"Testing\"):\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                y_pred = model(X)\n",
    "\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                test_loss += loss.detach().cpu().item()/len(val_loader)\n",
    "\n",
    "                correct += torch.sum(torch.argmax(y_pred, dim=1)==y).detach().cpu().item()\n",
    "                total += len(X)\n",
    "            print(f\"Test loss: {test_loss:.3f}\")\n",
    "            print(f\"Test accuracy: {correct/total*100:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb501c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTModel, self).__init__()\n",
    "\n",
    "    def foward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72168147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patches(images, num_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w\n",
    "\n",
    "    patches = torch.zeros(n, num_patches**2, h*w*c//num_patches**2)\n",
    "    patch_size = h//num_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in num_patches:\n",
    "            for j in num_patches:\n",
    "                patch = image[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
    "                patches[idx, i*num_patches+j] = patch.flatten()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6447f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f961db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit_env_two",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
